# Models 

Processed data and pretrained models will be available later, along with details on how to train and load the models.

This directory contains the model definitions and scripts to train, evaluate and compare the models.

# Generation models (speaker):
We use a different naming convention in the repository as compared to paper:

- Ref - model_speaker_base.py
- ReRef - model_speaker_hist_att.py
- Copy - model_speaker_hist_att_COPY.py

*(The baseline version of ReRef uses the first generated utterance in all later mentions. It is not a trained model, as it is simply copying the first generated outputs from the ReRef model. You can find its implementation in baseline_from_generated1st_REREF.py.)*

In speaker/utils/new_dataset_processor.py, we provide the code for processing the utterance-based reference chains for Ref and ReRef generation models. For the Copy model, however, we use speaker/utils/new_dataset_processor_copy.py, as it needs to keep track of the original forms of <unk> tokens. 

Generated outputs from the 5 runs of models along with their contexts for the test sets are obtained in compare_ref_hyp_* scripts and provided in:

- generated_outputs_model_all_base_TEST.txt
- generated_outputs_model_all_copy_TEST.txt
- generated_outputs_model_all_histatt_TEST.txt

pretrained_speaker* scripts load trained generation models and evaluate them on the test set.

# Reference resolution models (listener):
We use a different naming convention in the repository as compared to paper:

- One-hot baseline - model_bert_BASELINE_1H.py
- Ablation - model_bert_att_ctx.py
- Proposed - model_bert_att_ctx_hist.py

In **listener/utils/new_dataset_processor_BERT.py**, we provide the code for processing the utterance-based reference chains (getting the image contexts, target image and linguistic context). Here, we also convert the linguistic input into BERT representations.

eval_* scripts run evaluations of various pretrained models depending on whether we are looking at only the first utterances or cases where the target image has history.


# Inputting generation model outputs into reference resolution models
predict_listener* scripts take in trained generation models and plug in their generations into the best reference resolution model. In this way, we investigate how discriminative the utterances generated by the generation models are. '_BREAKDOWN' versions look at first and later utterances separately.
 

